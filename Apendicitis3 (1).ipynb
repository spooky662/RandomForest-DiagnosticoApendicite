{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1KPpm2e4aVEKU9eNALn4CJhckIIqEqT8m","authorship_tag":"ABX9TyNC/ua8Plu5Cch+EWWeUZX5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gONucgpJtDLx","executionInfo":{"status":"ok","timestamp":1749256351072,"user_tz":180,"elapsed":19801,"user":{"displayName":"Yasmin Fragoso de Camargo","userId":"07357058578350446140"}},"outputId":"57e4be32-fa86-44b6-e579-a63b9542d5f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======= Variável alvo: Diagnosis =======\n","Distribuição original: Counter({0: 463, 1: 317, 2: 2})\n","Distribuição após RandomOverSampler: Counter({0: 463, 1: 463, 2: 463})\n","Acurácia com Hold-Out: 0.9784172661870504\n","Acurácia com Cross-Validation: 0.9741007194244606\n","Melhores hiperparâmetros:\n","{'criterion': 'entropy',\n"," 'max_depth': 9,\n"," 'max_features': 'sqrt',\n"," 'splitter': 'random'}\n","\n","======= Variável alvo: Severity =======\n","Distribuição original: Counter({1: 662, 0: 119, 2: 1})\n","Distribuição após RandomOverSampler: Counter({1: 662, 2: 662, 0: 662})\n","Acurácia com Hold-Out: 0.9848993288590604\n","Acurácia com Cross-Validation: 0.9732373991167961\n","Melhores hiperparâmetros:\n","{'criterion': 'gini',\n"," 'max_depth': 9,\n"," 'max_features': 'sqrt',\n"," 'splitter': 'best'}\n","\n","======= Variável alvo: Management =======\n","Distribuição original: Counter({0: 483, 1: 270, 2: 27, 4: 1, 3: 1})\n","Distribuição após RandomOverSampler: Counter({0: 483, 4: 483, 1: 483, 2: 483, 3: 483})\n","Acurácia com Hold-Out: 0.9751724137931035\n","Acurácia com Cross-Validation: 0.9797263468330991\n","Melhores hiperparâmetros:\n","{'criterion': 'gini',\n"," 'max_depth': 9,\n"," 'max_features': 'sqrt',\n"," 'splitter': 'best'}\n"]}],"source":["import pandas as pd\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","from collections import Counter\n","from pprint import pprint\n","\n","# Carregamento dos dados\n","dados = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Appendicitis/app_data - All cases.csv\", encoding=\"ISO-8859-1\")\n","\n","# Guardar as variáveis-alvo separadamente\n","variaveis_alvo = [\"Diagnosis\", \"Severity\", \"Management\"]\n","dados_alvos = dados[variaveis_alvo].copy()\n","\n","# Pré-processamento leve\n","for col in dados.columns:\n","    if dados[col].dtype == \"object\" and col not in variaveis_alvo:\n","        dados[col] = dados[col].str.replace(\",\", \".\", regex=False)\n","        try:\n","            dados[col] = dados[col].astype(float)\n","        except:\n","            pass\n","\n","# Preencher valores ausentes\n","dados = dados.fillna(dados.mean(numeric_only=True))\n","\n","# Codificar variáveis-alvo com LabelEncoder\n","alvos_codificados = {}\n","for alvo in variaveis_alvo:\n","    le = LabelEncoder()\n","    dados_alvos[alvo] = le.fit_transform(dados_alvos[alvo])\n","    alvos_codificados[alvo] = le\n","\n","# Remover as colunas-alvo antes de dummificar\n","dados_sem_alvo = dados.drop(columns=variaveis_alvo)\n","dados_dummies = pd.get_dummies(dados_sem_alvo)\n","\n","# Juntar os dados dummificados com os alvos codificados\n","dados_final = pd.concat([dados_dummies, dados_alvos], axis=1)\n","\n","# Função para treinar modelo para cada variável-alvo usando RandomOverSampler\n","def treinar_modelo_para_alvo(alvo):\n","    print(f\"\\n======= Variável alvo: {alvo} =======\")\n","    dados_copia = dados_final.copy()\n","\n","    if alvo not in dados_copia.columns:\n","        raise ValueError(f\"Coluna '{alvo}' não encontrada no DataFrame.\")\n","\n","    # Separar atributos e classes\n","    dados_atributos = dados_copia.drop(columns=[alvo])\n","    dados_classes = dados_copia[alvo]\n","\n","    # Verificar balanceamento original\n","    print(\"Distribuição original:\", Counter(dados_classes))\n","\n","    # Balancear com RandomOverSampler (sem necessidade de k_neighbors)\n","    ros = RandomOverSampler(random_state=42)\n","    atributos_b, classes_b = ros.fit_resample(dados_atributos, dados_classes)\n","\n","    print(\"Distribuição após RandomOverSampler:\", Counter(classes_b))\n","\n","    # Treinar e avaliar modelo com Hold-Out\n","    attr_train, attr_test, class_train, class_test = train_test_split(\n","        atributos_b, classes_b, test_size=0.3, random_state=42\n","    )\n","    tree = DecisionTreeClassifier(random_state=42)\n","    tree.fit(attr_train, class_train)\n","    predictions = tree.predict(attr_test)\n","    print(\"Acurácia com Hold-Out:\", accuracy_score(class_test, predictions))\n","\n","    # Cross-Validation\n","    scores = cross_val_score(tree, atributos_b, classes_b, cv=10)\n","    print(\"Acurácia com Cross-Validation:\", scores.mean())\n","\n","    # Otimização de hiperparâmetros\n","    grid = {\n","        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n","        \"splitter\": [\"best\", \"random\"],\n","        \"max_depth\": [3, 6, 9],\n","        \"max_features\": [\"sqrt\", \"log2\"],\n","    }\n","\n","    search = RandomizedSearchCV(tree, grid, cv=10, random_state=42, n_jobs=-1)\n","    search.fit(atributos_b, classes_b)\n","\n","    print(\"Melhores hiperparâmetros:\")\n","    pprint(search.best_params_)\n","\n","    # Treinar modelo final com melhores parâmetros\n","    modelo_final = DecisionTreeClassifier(**search.best_params_)\n","    modelo_final.fit(atributos_b, classes_b)\n","\n","    return modelo_final\n","\n","# Treinar modelos para cada variável-alvo\n","modelo_diagnosis = treinar_modelo_para_alvo(\"Diagnosis\")\n","modelo_severity = treinar_modelo_para_alvo(\"Severity\")\n","modelo_management = treinar_modelo_para_alvo(\"Management\")\n"]}]}